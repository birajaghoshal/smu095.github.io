{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(\"../src/fastai/\")\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import fastai\n",
    "from fastai.imports import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inference framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Custom Monte Carlo dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMC(nn.Module):\n",
    "    r\"\"\"\n",
    "    Modified version of Dropout from torch/nn/modules/dropout.py\n",
    "    Args:\n",
    "        p: probability of an element to be zeroed. Default: 0.5\n",
    "        dropoutMC: If set to ``True``, dropout is turned on at test time. Default: ``True`\n",
    "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "    Shape:\n",
    "        - Input: `Any`. Input can be of any shape\n",
    "        - Output: `Same`. Output is of the same shape as input\n",
    "    Examples::\n",
    "        >>> m = nn.Dropout(p=0.2)\n",
    "        >>> input = autograd.Variable(torch.randn(20, 16))\n",
    "        >>> output = m(input)\n",
    "    .. _Improving neural networks by preventing co-adaptation of feature\n",
    "        detectors: https://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5, dropoutMC=True, inplace=False):\n",
    "        super(DropoutMC, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.dropoutMC = dropoutMC\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.dropout(input, self.p, self.dropoutMC, self.inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) \\\n",
    "            + inplace_str + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Sampling procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(learner, data, T=100):\n",
    "    ''' Function that gathers all relevant numerical results from MC dropout over T iterations.\n",
    "        \n",
    "        Arguments:\n",
    "        learner, fastai learner object\n",
    "        data, fastai dataloader\n",
    "        T, number of stochastic forward passes\n",
    "    '''\n",
    "    # Get images, labels and filenames\n",
    "    imgs, labels = next(iter(data.val_dl))\n",
    "    fnames = data.val_ds.fnames\n",
    "\n",
    "    # Empty dictionary to store all output\n",
    "    output = {}\n",
    "\n",
    "    # Empty array to store results in\n",
    "    results = np.empty((T, num_classes))\n",
    "\n",
    "    # iterator index to keep in dictionary\n",
    "    k=0\n",
    "\n",
    "    for (img, label, fname) in list(zip(imgs, labels, fnames)):\n",
    "\n",
    "        for i in range(T):\n",
    "            prediction = learner.predict_array(img[None])\n",
    "            results[i] = prediction\n",
    "\n",
    "        probs = to_np(F.softmax(V(results)))\n",
    "        probs_mean = np.mean(probs, axis=0)\n",
    "        pred_std = np.std(probs, axis=0)\n",
    "\n",
    "        prediction = probs_mean.argmax()\n",
    "        uncertainty = pred_std[prediction]\n",
    "\n",
    "        correct = 1 if prediction == label else 0\n",
    "\n",
    "        output[k] = {\"img\": fname, \"softmax_dist\": probs, \"probs\": probs_mean, \"prediction\": prediction, \"truth\": label, \"uncertainty\": uncertainty, \"correct\": correct}\n",
    "        k+=1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet architecture (kernel size = 5)\n",
    "class lenet_all_size5(nn.Module):\n",
    "    def __init__(self, conv_size=5, pool_size=2, drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=192, kernel_size=conv_size)\n",
    "        self.dropmc1 = DropoutMC(p=self.drop_rate)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=192, out_channels=192, kernel_size=conv_size, padding=2)\n",
    "        self.dropmc2 = DropoutMC(p=self.drop_rate)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        self.dense1 = nn.Linear(in_features=7*7*192, out_features=1000)\n",
    "        self.dropmc3 = DropoutMC(p=self.drop_rate)\n",
    "        self.dense2 = nn.Linear(in_features=1000, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropmc1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropmc2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropmc3(F.relu(self.dense1(x)))\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# LeNet architecture (kernel size=3)\n",
    "class lenet_all_size3(nn.Module):\n",
    "    def __init__(self, conv_size=3, pool_size=2, drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=192, kernel_size=conv_size)\n",
    "        self.dropmc1 = DropoutMC(p=self.drop_rate)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=192, out_channels=192, kernel_size=conv_size, padding=2)\n",
    "        self.dropmc2 = DropoutMC(p=self.drop_rate)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        self.dense1 = nn.Linear(in_features=8*8*192, out_features=1000)\n",
    "        self.dropmc3 = DropoutMC(p=self.drop_rate)\n",
    "        self.dense2 = nn.Linear(in_features=1000, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropmc1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropmc2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropmc3(F.relu(self.dense1(x)))\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(img_sz, batch_sz, stats):\n",
    "    # Normalising and augmenting data\n",
    "    tfms = tfms_from_stats(stats, img_sz, aug_tfms=transforms_side_on)\n",
    "    return ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sampling results into pandas data frame\n",
    "def get_pd_results(learner, data):\n",
    "    results = inference(learner, data)\n",
    "    pd_results = pd.DataFrame(results).T\n",
    "    return pd_results\n",
    "\n",
    "# Function for getting highest probability\n",
    "def get_prob(x):\n",
    "    return np.sort(x)[-1]\n",
    "\n",
    "# Function for getting second highest probability\n",
    "def get_2nd_prob(x):\n",
    "    return np.sort(x)[-2]\n",
    "\n",
    "# Function for getting second highest class\n",
    "def get_2nd_class(x):\n",
    "    return np.argsort(x)[-2]\n",
    "\n",
    "# Logit transformation\n",
    "def logit(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "# Gathering feature engineering in function\n",
    "def new_features(df):\n",
    "    new_df = df\n",
    "    # Getting softmax of predicted class (prob1) and runner-up (prob2), and class of runner-up (class2)\n",
    "    new_df[\"prob1\"], new_df[\"prob2\"], new_df[\"class2\"]  = new_df[\"probs\"].apply(get_prob), new_df[\"probs\"].apply(get_2nd_prob), new_df[\"probs\"].apply(get_2nd_class)\n",
    "    # Getting difference in softmax output between predicted class and class with second highest softmax\n",
    "    new_df[\"diff\"] = new_df[\"prob1\"] - new_df[\"prob2\"]\n",
    "    # Ratio of difference in softmax outputs/uncertainty estimation for predicted class\n",
    "    new_df[\"diff_sd_ratio\"] = new_df[\"diff\"]/new_df[\"uncertainty\"]\n",
    "    # Logit transformation of softmax\n",
    "    new_df[\"logit_prob1\"] = new_df[\"prob1\"].apply(logit)\n",
    "    return new_df\n",
    "\n",
    "# Function preparing data for R analysis\n",
    "def prepare_R(df):\n",
    "    return df.drop(columns=[\"img\", \"probs\", \"softmax_dist\"])\n",
    "\n",
    "# Prepare data for plotting\n",
    "def prepare_img(df):\n",
    "    return df[[\"img\", \"prediction\", \"softmax_dist\", \"prob1\", \"prob2\", \"class2\", \"truth\", \"uncertainty\"]]\n",
    "\n",
    "# Return 5 most uncertain\n",
    "def most_uncertain(df, status, order):\n",
    "    return df[df[\"correct\"]==status].sort_values(by=\"uncertainty\", ascending=order)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_plots(df, classes, status, order, save=False):\n",
    "    ''' Function that generates plots including image, scatterplot of softmax values for \n",
    "        predicted class and kernel density estimate of distribution of softmax values.\n",
    "        \n",
    "        Arguments:\n",
    "        df, pandas data frame\n",
    "        classes (str), tuple of class labels\n",
    "        status (int), 0: incorrect classification or 1: correct classification\n",
    "        order (bool), False: sort by most uncertain or True: sort by most certain\n",
    "        '''\n",
    "\n",
    "    # Getting predictions\n",
    "    df = most_uncertain(df, status, order)\n",
    "    df = prepare_img(df)\n",
    "\n",
    "    # Getting values for each image from each row in df\n",
    "    for idx, row in df.iterrows():\n",
    "        file_name = row[\"img\"]\n",
    "        prediction = row[\"prediction\"]\n",
    "        dist = row[\"softmax_dist\"]\n",
    "        pred_mean = row[\"prob1\"]\n",
    "        p_runnerup = row[\"prob2\"] # Added\n",
    "        runnerup = row[\"class2\"] # Added\n",
    "        truth = row[\"truth\"]\n",
    "        uncertainty = row[\"uncertainty\"]\n",
    "\n",
    "        # Saving softmax values of predicted class for all 100 iterations\n",
    "        softmax = []\n",
    "        for obs in dist:\n",
    "            softmax.append(obs[prediction])\n",
    "\n",
    "        # Getting image\n",
    "        img_path = PATH + \"/\" + file_name\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Creating figure\n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plotting image with true label\n",
    "        fig.add_subplot(131)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Label: {classes[truth]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Scatterplot of prediction pr. iteration\n",
    "        fig.add_subplot(132)\n",
    "        x = range(0, len(softmax))\n",
    "        plt.scatter(x,softmax)\n",
    "        if prediction == truth:\n",
    "            plt.title(f\"Prediction: [{classes[prediction]}], Runner-up: [{classes[runnerup]}]\", color=\"g\")\n",
    "        else:\n",
    "            plt.title(f\"Prediction: [{classes[prediction]}], Runner-up: [{classes[runnerup]}]\", color=\"r\")\n",
    "        plt.axhline(pred_mean, color=\"r\", linewidth=1, label=\"Prediction\")\n",
    "        plt.axhline(p_runnerup, color=\"sienna\", ls=\"dashed\", linewidth=1, label=\"Runner-up\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(f\"p({prediction}|x,w)\")\n",
    "\n",
    "        # Kernel density plot of ECDF predictions after T forward passes\n",
    "        fig.add_subplot(133)\n",
    "        sns.kdeplot(softmax, shade=True, color=\"b\")\n",
    "        plt.xlim(0,1)\n",
    "        plt.title(\"KDE: $\\mu$={:.4f}, $\\sigma$={:.4f}\".format(pred_mean, uncertainty))\n",
    "        \n",
    "        if save:\n",
    "            fig.savefig(f\"./figs/{idx}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "PATH=\"../../master/DAT259/data/fastai-data/cifar10\"\n",
    "\n",
    "# Class labels\n",
    "classes = (\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "num_classes = 10\n",
    "\n",
    "# Image and batch sizes\n",
    "img_size = 32\n",
    "batch_size = 128\n",
    "\n",
    "# Mean and std. dev. of individual color channels for all CIFAR-10 (taken from Jeremy's notebook)\n",
    "stats = (np.array([0.4914 ,  0.48216,  0.44653]), np.array([0.24703,  0.24349,  0.26159]))\n",
    "\n",
    "data = get_data(img_size, batch_size, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastai` provides a wrapper around PyTorch which vastly simplifies the training process. This saves time for simple models such as `lenet-all` (and to a certain extent the predesigned models accompanying PyTorch). For all it's practicality, the `fastai` code is somewhat convoluted and poorly documented. This makes it hard and time-consuming to do other tasks. It is therefore a useful tool for quick prototyping, but for more complicated models and procedures it seems that learning the proper PyTorch approach is well worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model 1: kernel size = 5, drop_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dropout rate\n",
    "p = 0.5\n",
    "\n",
    "# Initialising model\n",
    "model = lenet_all_size5(drop_rate=p)\n",
    "\n",
    "# Initialising learner\n",
    "learner = ConvLearner.from_model_data(model, data)\n",
    "\n",
    "# Setting loss function (fastai default is negative log-likelihood)\n",
    "learner.crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Finding base learning rate\n",
    "learner.lr_find()\n",
    "learner.sched.plot()\n",
    "\n",
    "# The base learning rate and weight decay of the network.\n",
    "base_lr = 1e-3\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Fitting model and saving best weights\n",
    "learner.fit(base_lr, n_cycle=5, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_p05\")\n",
    "learner.fit(base_lr, n_cycle=5, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_p05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model 2: kernel size = 5, drop_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dropout rate\n",
    "p = 0.2\n",
    "\n",
    "# Initialising model\n",
    "model = lenet_all_size5(drop_rate=p)\n",
    "\n",
    "# Initialising learner\n",
    "learner = ConvLearner.from_model_data(model, data)\n",
    "\n",
    "# Setting loss function (fastai default is negative log-likelihood)\n",
    "learner.crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Finding base learning rate\n",
    "learner.lr_find()\n",
    "learner.sched.plot()\n",
    "\n",
    "# The base learning rate and weight decay of the network.\n",
    "base_lr = 1e-2\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Fitting model and saving best weights\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_p02\")\n",
    "learner.fit(1e-4, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_p02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model 3: kernel size = 3, drop_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dropout rate\n",
    "p = 0.5\n",
    "\n",
    "# Initialising model\n",
    "model = lenet_all_size3(drop_rate=p)\n",
    "\n",
    "# Initialising learner\n",
    "learner = ConvLearner.from_model_data(model, data)\n",
    "\n",
    "# Setting loss function (fastai default is negative log-likelihood)\n",
    "learner.crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Finding base learning rate\n",
    "learner.lr_find()\n",
    "learner.sched.plot()\n",
    "\n",
    "# The base learning rate and weight decay of the network.\n",
    "base_lr = 1e-3\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Fitting model and saving best weights\n",
    "learner.fit(base_lr, n_cycle=5, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model 4: kernel_size = 3, drop_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dropout rate\n",
    "p = 0.2\n",
    "\n",
    "# Initialising model\n",
    "model = lenet_all_size3(drop_rate=p)\n",
    "\n",
    "# Initialising learner\n",
    "learner = ConvLearner.from_model_data(model, data)\n",
    "\n",
    "# Setting loss function (fastai default is negative log-likelihood)\n",
    "learner.crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Finding base learning rate\n",
    "learner.lr_find()\n",
    "learner.sched.plot()\n",
    "\n",
    "# The base learning rate and weight decay of the network.\n",
    "base_lr = 1e-3\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Fitting model and saving best weights\n",
    "learner.fit(base_lr, n_cycle=5, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p02\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p02\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p02\")\n",
    "learner.fit(base_lr, n_cycle=3, cycle_len=1, cycle_mult=2, wds=weight_decay, best_save_name=\"lenet_all_conv3_p02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising models\n",
    "model_conv5_drop5 = ConvLearner.from_model_data(lenet_all_size5(), data)\n",
    "model_conv5_drop2 = ConvLearner.from_model_data(lenet_all_size5(drop_rate=0.2), data)\n",
    "model_conv3_drop5 = ConvLearner.from_model_data(lenet_all_size3(), data)\n",
    "model_conv3_drop2 = ConvLearner.from_model_data(lenet_all_size3(drop_rate=0.2), data)\n",
    "\n",
    "# Loading pretrained weights\n",
    "model_conv5_drop5.load(\"lenet_all_p05\")\n",
    "model_conv5_drop2.load(\"lenet_all_p02\")\n",
    "model_conv3_drop5.load(\"lenet_all_conv3_p05\")\n",
    "model_conv3_drop2.load(\"lenet_all_conv3_p02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference and pickling results\n",
    "results_df1 = get_pd_results(model_conv5_drop5, data_test)\n",
    "results_df1.to_pickle(\"model_conv5_drop5-v2.pkl\")\n",
    "\n",
    "results_df2 = get_pd_results(model_conv5_drop2, data_test)\n",
    "results_df2.to_pickle(\"model_conv5_drop2-v2.pkl\")\n",
    "\n",
    "results_df3 = get_pd_results(model_conv3_drop5, data_test)\n",
    "results_df3.to_pickle(\"model_conv3_drop5-v2.pkl\")\n",
    "\n",
    "results_df4 = get_pd_results(model_conv3_drop2, data_test)\n",
    "results_df4.to_pickle(\"model_conv3_drop2-v2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty",
   "language": "python",
   "name": "uncertainty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
